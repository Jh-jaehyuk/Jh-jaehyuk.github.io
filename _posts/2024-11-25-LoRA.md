---
title: Paper - Low-Rank Adaptation of Large Language Models
date: 2024-11-27 17:30:35 +09:00
categories: [Paper, LoRA]
tags: 
    [
        Paper,
        Transformer,
        LLM,
        Large Language Model,
        LoRA,
        PEFT
    ]
use_math: true
---

# LoRA: Low-Rank Adaptation of Large Language Models

## Introduction
현대의 대규모 언어모델(LLM)은 뛰어난 성능을 보이지만, 특정 작업으로 파인튜닝하려면
막대한 계산 비용과 저장 공간이 필요합니다. 특히 모든 모델 파라미터를 학습해야하는
기존 방식을 비효율적일 뿐 아니라, 여러 작업을 지원하는 멀티태스킹에도 제약이 있습니다.  
이를 해결하기 위해 저자들이 제안한 것이 **LoRA(Low-Rank Adaptation)** 입니다.  
**LoRA**는 **PEFT(Parameter Efficient Fine-Tuning)** 기술 중 하나로, 
적은 파라미터만 학습하면서도 높은 성능을 유지할 수 있는 접근 방법입니다.  
![introduction](https://github.com/user-attachments/assets/66618652-b4b2-4f53-a33e-1a52b74d9ff9)  

## Background
기존의 PEFT 방법으로는 다음 두 가지가 주목받았습니다.  

### 1. Adapter Layers  
Transformer 블록에 작은 네트워크를 추가해 학습하는 방법  
그러나 모델 구조가 복잡해지고 추가 연산 비용이 발생하는 문제가 있음

### 2. Input Activation Optimization  
입력 데이터를 학습 가능한 방식으로 변환하는 방식  
하지만 입력 조정만으로는 모델 전체 표현력을 크게 변화시키기 어려움

LoRA는 이 두가지 방법론에서 더 나아가, 모델 가중치 공간에서
**저차원 변화만을 학습함으로써 계산 비용과 성능 간 균형을 최적화**합니다.

## Method
LoRA의 핵심은 Transformer 모델의 가중치 행렬 $W$에 추가적인 변화량(Delta Weight)을
**Low-Rank Decomposition**으로 표현하는 데 있습니다.  
$$W' = W + \Delta W \ \ \ \ \ \ \text{where}\ \ \Delta W = A \cdot B$$  
여기서 $A \in \mathbb{R}^{d\times r}$와 $B \in \mathbb{R}^{r\times k}$는
저차원 행렬로, $r \ll \text{min}(d,k)$일 때 학습해야 할 파라미터 수는
$r \cdot (d+k)$로 크게 감소합니다. 이로 인해 메모리 효율성을 유지하면서도 작업에 특화된
가중치 조정을 효과적으로 수행할 수 있습니다.

## Feature
LoRA의 주요 특징은 다음과 같습니다.  
* **효율성:** 학습해야 할 파라미터 수와 메모리 사용량을 크게 줄임
* **모델 구조 유지:** 원래 가중치 $W$는 고정되므로, 기존 모델을 손상시키지 않고 추가 학습 가능
* **멀티태스킹 지원:** 각 작업에 맞는 $\Delta W$만 학습하면 하나의 모델로 여러 작업을 처리할 수 있음  

위와 같은 특징들 덕분에, LoRA는 자원 제약이 큰 환경에서도 대규모 모델을 활용할 수 있는
유망한 기술로 주목받고 있습니다.  

## Result
논문에서 GPT-3와 BERT 기반 모델에 LoRA를 적용해 실험한 결과,
**파라미터 크기를 30% 미만으로 줄이면서**도 **성능은 기존 파인튜닝 방식과 유사**한 수준을 유지하였고
**Adapter Layers와 비교해 연산량이 감소하며 속도와 메모리 사용 효율에서 앞서는 모습**을 보여주었습니다.  
이 실험을 통해 LoRA는 대규모 모델의 학습 효율성을 획기적으로 향상시키는 방법임을 입증했습니다.

## Limitation
**LoRA는 자연어 처리(NLP)뿐 아니라, 이미지 생성, 음성 인식 등 다양한 분야에서 활용**될 수 있습니다.  
다만, **모든 작업에 대해 최적의 랭크($r$)를 설정하는 것이 쉽지 않으며**, 
매우 큰 모델에서는 초기화 비용이 약간 증가할 수 있습니다.  
저자들은 이를 해결하기 위한 후속 연구도 진행 중이라고 덧붙였습니다.

## Conclusion
**LoRA는 대규모 모델의 학습 비용 문제를 해결하면서도 성능을 유지하는 효과적인 방법**입니다.  
앞으로 LoRA가 다른 모델 아키텍처에도 확장되거나, 자동화된 랭크 선택 기법과
결합된다면 더욱 강력한 도구로 자리 잡을 것입니다.